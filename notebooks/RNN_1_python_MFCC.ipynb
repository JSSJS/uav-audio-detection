{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "# Math\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "uav_path = '../../data/22050/loaded/*.*'\n",
    "loaded_path = '../../data/22050/unloaded/*.*'\n",
    "none_path = '../../data/22050/none/*.*'\n",
    "\n",
    "uav_files = glob.glob(uav_path)\n",
    "loaded_files = glob.glob(loaded_path)\n",
    "none_files = glob.glob(none_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 개\t ../../data/22050/loaded\\LGgram_P2_loaded bck and forth.wav\n",
      "10 개\t ../../data/22050/unloaded\\LGgram_P2_unloaded back forth.wav\n",
      "25 개\t ../../data/22050/none\\background_06_02_01.WAV\n"
     ]
    }
   ],
   "source": [
    "print(len(uav_files),'개\\t', uav_files[0])\n",
    "print(len(uav_files),'개\\t', loaded_files[0])\n",
    "print(len(none_files), '개\\t',none_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "The reason of why SR is 44100 is that the sample rate of above files is 44.1kbps\n",
    "\n",
    "a wav file sample has 884736. if sample is divided by sample rate, the value is time\n",
    "the time is fixed by 20.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(files, sr=22050):\n",
    "    [raw, sr] = librosa.load(files[0], sr=sr)\n",
    "    for f in files[1:]:\n",
    "        [array, sr] = librosa.load(f, sr=sr)\n",
    "        raw = np.hstack((raw, array))\n",
    "    print(raw.shape)\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14077547,)\n",
      "(23781589,)\n",
      "(10960896,)\n"
     ]
    }
   ],
   "source": [
    "uav_raw = load(uav_files)\n",
    "loaded_raw = load(loaded_files)\n",
    "none_raw = load(none_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction \n",
    "## steps\n",
    "#### 1. Resampling \n",
    "#### 2. *VAD*( Voice Activity Detection)\n",
    "#### 3. Maybe padding with 0 to make signals be equal length\n",
    "#### 4. Log spectrogram (or *MFCC*, or *PLP*)\n",
    "#### 5. Features normalization with *mean* and *std*\n",
    "#### 6. Stacking of a given number of frames to get temporal information\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Resampling\n",
    "\n",
    "if you see the graph, there are few at high frequency. this is mean that data is big but it's no useless. so To small the data, do Resampling. In general, use 0~8000Hz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. VAD\n",
    "\n",
    "Sometimes, Files have silence. It is not necessary. So, We need to find sound of Drone except silence.\n",
    "\n",
    "But, Not yet implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. padding with 0 to make signals be equal length\n",
    "\n",
    "If we have a lot of sound files, we need to pad some datas. But These files's time is longger than 1 second. So It dosn't need to pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Log spectrogram (or MFCC, or PLP)\n",
    "\n",
    "The upper picture is resampled data. \n",
    "The lower picture is original data.\n",
    "\n",
    "In MFCC Feature, There is no big difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "#returns mfcc features with mean and standard deviation along time\n",
    "def mfcc4(raw, label, chunk_size=8192, window_size=4096, sr=22050, n_mfcc=16, n_frame=16):\n",
    "    mfcc = np.empty((0, n_mfcc* n_frame))\n",
    "    y = []\n",
    "    print(raw.shape)\n",
    "    for i in range(0, len(raw), chunk_size//2):\n",
    "        mfcc_slice = librosa.feature.mfcc(raw[i:i+chunk_size], sr=sr, n_mfcc=n_mfcc) #n_mfcc,17\n",
    "        if mfcc_slice.shape[1] < 17:\n",
    "            print(\"small end:\", mfcc_slice.shape)\n",
    "            continue\n",
    "        mfcc_slice = mfcc_slice[:,:-1]\n",
    "        #print(mfcc_slice.shape)\n",
    "        mfcc_slice = mfcc_slice.reshape((1, mfcc_slice.shape[0]* mfcc_slice.shape[1]))\n",
    "        #print(mfcc_slice.shape)\n",
    "        mfcc = np.vstack((mfcc, mfcc_slice))\n",
    "        y.append(label)\n",
    "    y = np.array(y)\n",
    "    return mfcc, y\n",
    "\n",
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "def mfcc(raw, chunk_size=8192, sr=22050, n_mfcc=13):\n",
    "    mfcc = np.empty((13, 0))\n",
    "    for i in range(0, len(raw), chunk_size):\n",
    "        mfcc_slice = librosa.feature.mfcc(raw[i:i+chunk_size], sr=sr, n_mfcc=n_mfcc)\n",
    "        mfcc = np.hstack((mfcc, mfcc_slice))\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mfcc_uav, y_uav = mfcc4(uav_raw, 1)\n",
    "print(mfcc_uav.shape, y_uav.shape)\n",
    "mfcc_none, y_none = mfcc4(none_raw, 0)\n",
    "print(mfcc_none.shape, y_none.shape)\n",
    "'''\n",
    "mfcc_uav = mfcc(uav_raw)\n",
    "mfcc_loaded = mfcc(loaded_raw)\n",
    "mfcc_none = mfcc(none_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29214,) 2\n",
      "(49352,) 1\n",
      "(22746,) 0\n",
      "(101312, 13) (101312,)\n"
     ]
    }
   ],
   "source": [
    "# or should we give one label to one chunk?\n",
    "y_uav = np.ones(mfcc_uav.shape[1], dtype=int)*2\n",
    "y_loaded = np.ones(mfcc_loaded.shape[1], dtype=int)\n",
    "y_none =np.zeros(mfcc_none.shape[1], dtype=int)\n",
    "\n",
    "print(y_uav.shape, y_uav[0])\n",
    "print(y_loaded.shape, y_loaded[0])\n",
    "print(y_none.shape, y_none[0])\n",
    "\n",
    "X = np.hstack((mfcc_uav, mfcc_loaded))\n",
    "X = np.hstack((X, mfcc_none)).T\n",
    "\n",
    "y = np.hstack((y_uav, y_loaded))\n",
    "y = np.hstack((y, y_none))\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101312, 3)\n",
      "[0. 0. 1.] [0. 1. 0.] [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "n_labels = y.shape[0]\n",
    "n_unique_labels = 3\n",
    "y_encoded = np.zeros((n_labels, n_unique_labels))\n",
    "y_encoded[np.arange(n_labels), y] = 1\n",
    "print(y_encoded.shape)\n",
    "print(y_encoded[0], y_encoded[40000],y_encoded[100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmfcc_uav_list = mfcc_uav.tolist()\\nmfcc_uav_list = mfcc_uav_list\\nfig = plt.figure(figsize=(15,9))\\nax = fig.add_subplot(1,1,1)\\nax.plot(np.linspace(0,len(mfcc_uav_list), len(mfcc_uav_list)),mfcc_uav_list)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "mfcc_uav_list = mfcc_uav.tolist()\n",
    "mfcc_uav_list = mfcc_uav_list\n",
    "fig = plt.figure(figsize=(15,9))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(np.linspace(0,len(mfcc_uav_list), len(mfcc_uav_list)),mfcc_uav_list)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Features normalization with *mean* and *std*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stacking of a given number of frames to get temporal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX = np.concatenate((mfcc_uav, mfcc_none), axis=0)\\nY = np.hstack((y_uav, y_none))\\nprint(X.shape, Y.shape)\\nX = np.reshape(X,(X.shape[0],-1))# 선범 \\n\\nn_labels = Y.shape[0]\\nn_unique_labels = 2\\ny_encoded = np.zeros((n_labels, n_unique_labels))\\ny_encoded[np.arange(n_labels), Y] = 1\\nprint(y_encoded.shape)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X = np.concatenate((mfcc_uav, mfcc_none), axis=0)\n",
    "Y = np.hstack((y_uav, y_none))\n",
    "print(X.shape, Y.shape)\n",
    "X = np.reshape(X,(X.shape[0],-1))# 선범 \n",
    "\n",
    "n_labels = Y.shape[0]\n",
    "n_unique_labels = 2\n",
    "y_encoded = np.zeros((n_labels, n_unique_labels))\n",
    "y_encoded[np.arange(n_labels), Y] = 1\n",
    "print(y_encoded.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "(101312, 13) (101312, 3)\n"
     ]
    }
   ],
   "source": [
    "dataX = X\n",
    "dataY = y_encoded\n",
    "print(y_encoded)\n",
    "print(dataX.shape, dataY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101297, 16, 13) (101297, 3)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 16 #layer\n",
    "X_hot_list= []\n",
    "#Y_hot = dataY[seq_length-1:].reshape(len(dataY[seq_length-1:]), 1)\n",
    "Y_hot_tmp = dataY[seq_length-1:]\n",
    "\n",
    "for i in range(0, dataX.shape[0] - seq_length+1):\n",
    "    _x = dataX[i:i + seq_length]\n",
    "    #if i<10:\n",
    "        #print(_x, \"->\", Y_hot_tmp[i])\n",
    "    X_hot_list.append(_x)\n",
    "\n",
    "X_hot = np.array(X_hot_list[:])\n",
    "Y_hot= Y_hot_tmp.reshape((len(Y_hot_tmp),n_unique_labels))\n",
    "print(X_hot.shape, Y_hot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3] [11, 12, 13]\n",
      "[4, 5, 6] [14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "class Data:\n",
    "    def __init__(self,X,Y,BatchSize):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.len = len(Y)\n",
    "        self.bs = BatchSize\n",
    "        self.bs_i = 0\n",
    "    def getBatchData(self):\n",
    "        s = self.bs_i\n",
    "        e = self.bs_i + self.bs\n",
    "        if e> self.len:\n",
    "            e -= self.len\n",
    "            result =  np.vstack((self.X[s:],self.X[:e])), np.vstack((self.Y[s:],self.Y[:e]))\n",
    "        else:\n",
    "            result =  self.X[s:e], self.Y[s:e]\n",
    "            \n",
    "        self.bs_i = e\n",
    "        return result\n",
    "dataX = [1,2,3,4,5,6,7,8]\n",
    "dataY = [11,12,13,14,15,16,17,18]\n",
    "D = Data(dataX, dataY,3)\n",
    "x, y = D.getBatchData()\n",
    "print(x,y)\n",
    "x, y = D.getBatchData()\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nX_dim = 256\\nX_train = np.zeros(shape=[0,9,X_dim],dtype=float)\\ny_train = np.zeros(shape=[0,n_unique_labels],dtype=float)\\nX_test = np.zeros(shape=[0,9,X_dim],dtype=float)\\ny_test = np.zeros(shape=[0,n_unique_labels],dtype=float)\\n\\nsplit_rate = 0.9\\nnone_len = len(y_none)\\nuav_len = len(X_hot) - none_len\\nprint('uav_len, none_len', uav_len,none_len)\\n\\ntrain_size = int(uav_len * split_rate)\\ntest_size = uav_len - train_size\\nbase = 0\\nprint('train_uav, test_uav',train_size,test_size)\\n\\nX_tr, X_te = np.array(X_hot[base:base+train_size]),np.array(X_hot[base+train_size:base+train_size+test_size])\\ny_tr, y_te = np.array(Y_hot[base:base+train_size]),np.array(Y_hot[base+train_size:base+train_size+test_size])\\nprint(X_tr.shape,X_te.shape)\\nprint(y_tr.shape,y_te.shape)\\n\\nX_train = np.vstack((X_train,X_tr))\\nX_test= np.vstack((X_test,X_te))\\ny_train= np.vstack((y_train,y_tr))\\ny_test= np.vstack((y_test,y_te))\\n\\ntrain_size = int(none_len * split_rate)\\ntest_size = none_len - train_size\\nbase = uav_len\\nprint('none',train_size,test_size)\\n\\nX_tr, X_te = np.array(X_hot[base:base+train_size]),np.array(X_hot[base+train_size:base+train_size+none_len])\\ny_tr, y_te = np.array(Y_hot[base:base+train_size]),np.array(Y_hot[base+train_size:base+train_size+none_len])\\nprint(X_tr.shape,X_te.shape)\\nprint(y_tr.shape,y_te.shape)\\n\\nprint(base+none_len)\\nX_train = np.vstack((X_train,X_tr))\\nX_test= np.vstack((X_test,X_te))\\ny_train= np.vstack((y_train,y_tr))\\ny_test= np.vstack((y_test,y_te))\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X_dim = 256\n",
    "X_train = np.zeros(shape=[0,9,X_dim],dtype=float)\n",
    "y_train = np.zeros(shape=[0,n_unique_labels],dtype=float)\n",
    "X_test = np.zeros(shape=[0,9,X_dim],dtype=float)\n",
    "y_test = np.zeros(shape=[0,n_unique_labels],dtype=float)\n",
    "\n",
    "split_rate = 0.9\n",
    "none_len = len(y_none)\n",
    "uav_len = len(X_hot) - none_len\n",
    "print('uav_len, none_len', uav_len,none_len)\n",
    "\n",
    "train_size = int(uav_len * split_rate)\n",
    "test_size = uav_len - train_size\n",
    "base = 0\n",
    "print('train_uav, test_uav',train_size,test_size)\n",
    "\n",
    "X_tr, X_te = np.array(X_hot[base:base+train_size]),np.array(X_hot[base+train_size:base+train_size+test_size])\n",
    "y_tr, y_te = np.array(Y_hot[base:base+train_size]),np.array(Y_hot[base+train_size:base+train_size+test_size])\n",
    "print(X_tr.shape,X_te.shape)\n",
    "print(y_tr.shape,y_te.shape)\n",
    "\n",
    "X_train = np.vstack((X_train,X_tr))\n",
    "X_test= np.vstack((X_test,X_te))\n",
    "y_train= np.vstack((y_train,y_tr))\n",
    "y_test= np.vstack((y_test,y_te))\n",
    "\n",
    "train_size = int(none_len * split_rate)\n",
    "test_size = none_len - train_size\n",
    "base = uav_len\n",
    "print('none',train_size,test_size)\n",
    "\n",
    "X_tr, X_te = np.array(X_hot[base:base+train_size]),np.array(X_hot[base+train_size:base+train_size+none_len])\n",
    "y_tr, y_te = np.array(Y_hot[base:base+train_size]),np.array(Y_hot[base+train_size:base+train_size+none_len])\n",
    "print(X_tr.shape,X_te.shape)\n",
    "print(y_tr.shape,y_te.shape)\n",
    "\n",
    "print(base+none_len)\n",
    "X_train = np.vstack((X_train,X_tr))\n",
    "X_test= np.vstack((X_test,X_te))\n",
    "y_train= np.vstack((y_train,y_tr))\n",
    "y_test= np.vstack((y_test,y_te))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_hot, Y_hot, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "traindata = Data(X_train,y_train,batch_size)\n",
    "testdata = Data(X_test,y_test,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81037, 16, 13) (20260, 16, 13)\n",
      "(81037, 3) (20260, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnp.save('../data/Xy/X_train2', X_train)\\nnp.save('../data/Xy/X_test2', X_test)\\nnp.save('../data/Xy/y_train2', y_train)\\nnp.save('../data/Xy/y_test2', y_test)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "np.save('../data/Xy/X_train2', X_train)\n",
    "np.save('../data/Xy/X_test2', X_test)\n",
    "np.save('../data/Xy/y_train2', y_train)\n",
    "np.save('../data/Xy/y_test2', y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nX_train = np.load('../data/Xy/X_train2.npy')\\nX_test = np.load('../data/Xy/X_test2.npy')\\ny_train = np.load('../data/Xy/y_train2.npy')\\ny_test = np.load('../data/Xy/y_test2.npy')\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X_train = np.load('../data/Xy/X_train2.npy')\n",
    "X_test = np.load('../data/Xy/X_test2.npy')\n",
    "y_train = np.load('../data/Xy/y_train2.npy')\n",
    "y_test = np.load('../data/Xy/y_test2.npy')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = batch_size\n",
    "num_classes = 13            #분류할 사전의 크기 \n",
    "\n",
    "learning_rate = 0.01\n",
    "sequence_length = seq_length #9         \n",
    "\n",
    "output_dim = n_unique_labels\n",
    "layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, sequence_length,num_classes], name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, [None, output_dim], name=\"Y\")\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=num_classes, state_is_tuple=True)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([cell]*layers, state_is_tuple= True)\n",
    "\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X,initial_state=initial_state,dtype=tf.float32)\n",
    "\n",
    "dense1 = tf.contrib.layers.fully_connected(outputs[:,-1], output_dim, activation_fn=None)\n",
    "dense2 = tf.layers.dense(inputs=dense1, units=output_dim, activation=tf.nn.relu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_pred= tf.layers.dense(inputs=dense2, units=output_dim)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Y_pred, labels=Y))\n",
    "lr = tf.placeholder(tf.float32,shape=(), name='learning_rate')\n",
    "train = tf.train.AdamOptimizer(lr).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-22.0271102 ,  44.45796608, -16.81921293,  28.58088165,\n",
       "        -20.21966691,  17.78120399,  -5.26962715,  15.44042813,\n",
       "          3.50293016,  12.11053418,   7.38301421,   1.43603353,\n",
       "          5.23186589],\n",
       "       [-28.8449711 ,  43.59542704, -14.87262236,  20.83758294,\n",
       "        -21.88811659,  17.84122881,  -5.88527807,  12.87017004,\n",
       "          5.3933842 ,  12.70081834,   2.15844202,   1.58403938,\n",
       "          7.7837879 ],\n",
       "       [-28.3032851 ,  44.79240549, -14.96570827,  22.73259997,\n",
       "        -21.59513053,  14.5566326 ,  -6.47910848,  15.18947468,\n",
       "          3.62236637,  11.43807572,   2.30163804,  -0.54407771,\n",
       "          4.21434993],\n",
       "       [-28.95163981,  46.27454382, -15.38487285,  26.22065461,\n",
       "        -22.13071886,   8.7152196 ,  -9.61704134,  11.44961277,\n",
       "          1.63580257,  13.93560792,   2.93375169,  -3.52067628,\n",
       "         -2.75554843],\n",
       "       [-29.37341523,  48.07906995, -18.34130497,  26.59050163,\n",
       "        -20.83925559,  11.3864304 ,  -8.60146463,  11.19378418,\n",
       "          3.31791381,  14.14016407,   3.08223954,   0.36655775,\n",
       "         -3.34358085],\n",
       "       [-28.67558246,  48.80283508, -20.08703431,  25.09770568,\n",
       "        -23.99060984,  13.11536892,  -7.8722155 ,  11.7040358 ,\n",
       "          2.81746716,   9.8002434 ,   1.48630679,   1.01266678,\n",
       "          1.13228225],\n",
       "       [-31.04829548,  44.11839067, -17.09409593,  27.72746794,\n",
       "        -23.8632588 ,  17.49302537,  -5.74594614,  11.42619119,\n",
       "         -2.64754117,   6.89648365,   1.53821988,  -2.62566497,\n",
       "          2.66356789],\n",
       "       [-36.44710128,  42.41181189, -14.33801717,  27.55816565,\n",
       "        -21.07696472,  23.74023439,  -6.19993064,  10.26326348,\n",
       "         -3.10927753,   5.58734401,   4.49529857,  -1.4976338 ,\n",
       "          1.7400541 ],\n",
       "       [-38.8411103 ,  45.07962681, -12.45384036,  27.42832387,\n",
       "        -19.01697851,  25.54100571,  -7.77782812,  14.23328837,\n",
       "          2.8710013 ,   7.23172546,   5.64917463,   0.09773958,\n",
       "          2.17530321],\n",
       "       [-40.04735912,  42.6879213 , -11.76219173,  27.90881658,\n",
       "        -18.62405578,  23.36193974,  -9.43204721,  15.56402598,\n",
       "          6.38932336,   8.59945359,   1.03774564,  -3.90720134,\n",
       "          0.64685717],\n",
       "       [-41.63745086,  41.00252221, -11.34404283,  25.47689946,\n",
       "        -21.39999291,  19.42496304,  -7.14572284,  15.51644639,\n",
       "          5.11973691,   6.4252621 ,   0.35294336,  -1.36029391,\n",
       "          2.58637084],\n",
       "       [-47.17555397,  41.34725019,  -9.6912873 ,  27.83017206,\n",
       "        -18.43497743,  19.98928764,  -9.18734414,  11.59140436,\n",
       "          4.97589215,   5.83951202,   2.09055422,   3.06904434,\n",
       "          2.12278926],\n",
       "       [-40.43552152,  47.66406186,  -8.06660116,  21.25165968,\n",
       "        -24.53338306,  19.36251635,  -6.77633914,  18.46599808,\n",
       "          5.65822705,   9.28086649,   7.5102268 ,  -2.1226678 ,\n",
       "          4.06261287],\n",
       "       [-37.59253371,  49.62455796, -12.41982356,  19.91372017,\n",
       "        -24.34105516,  17.91462105,  -8.20855637,  19.71843523,\n",
       "          5.47278157,  10.68263057,   2.126804  ,  -2.18271355,\n",
       "          2.91685671],\n",
       "       [-40.03194528,  53.77475207,  -9.59476189,  24.2999092 ,\n",
       "        -22.38376483,  21.04301227,  -8.2096058 ,  20.38818715,\n",
       "          1.82728351,   5.77315976,  -3.27306834,  -1.97130252,\n",
       "          2.56147154],\n",
       "       [-45.25767023,  53.62831065,  -6.20606512,  24.07673642,\n",
       "        -22.01456826,  21.59608129, -11.70551624,  16.84563483,\n",
       "         -2.81208069,   8.59924758,   4.625906  ,  -0.46019816,\n",
       "          2.03994348]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata.X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(traindata.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x, y = traindata.getBatchData()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 1.1106698513031006\n",
      "[step: 1] loss: 1.091935396194458\n",
      "[step: 2] loss: 1.0793190002441406\n",
      "[step: 3] loss: 1.0684373378753662\n",
      "[step: 4] loss: 1.0580294132232666\n",
      "[step: 5] loss: 1.044013500213623\n",
      "[step: 6] loss: 1.0370820760726929\n",
      "[step: 7] loss: 1.0343177318572998\n",
      "[step: 8] loss: 1.0405983924865723\n",
      "[step: 9] loss: 1.0432360172271729\n",
      "[step: 10] loss: 1.0144588947296143\n",
      "[step: 11] loss: 1.0256236791610718\n",
      "[step: 12] loss: 1.0097323656082153\n",
      "[step: 13] loss: 0.9795637726783752\n",
      "[step: 14] loss: 0.9933551549911499\n",
      "[step: 15] loss: 0.9402954578399658\n",
      "[step: 16] loss: 0.9755253791809082\n",
      "[step: 17] loss: 0.9795761108398438\n",
      "[step: 18] loss: 0.9172914028167725\n",
      "[step: 19] loss: 0.9505681991577148\n",
      "[step: 20] loss: 0.9257932305335999\n",
      "[step: 21] loss: 0.9441235065460205\n",
      "[step: 22] loss: 0.8700234889984131\n",
      "[step: 23] loss: 0.8469570875167847\n",
      "[step: 24] loss: 0.862381637096405\n",
      "[step: 25] loss: 0.881547212600708\n",
      "[step: 26] loss: 0.8764456510543823\n",
      "[step: 27] loss: 0.8498073220252991\n",
      "[step: 28] loss: 0.8480000495910645\n",
      "[step: 29] loss: 0.8216224908828735\n",
      "[step: 30] loss: 0.8053390979766846\n",
      "[step: 31] loss: 0.7657448649406433\n",
      "[step: 32] loss: 0.7543630599975586\n",
      "[step: 33] loss: 0.7525343894958496\n",
      "[step: 34] loss: 0.6954213976860046\n",
      "[step: 35] loss: 0.6951543688774109\n",
      "[step: 36] loss: 0.689102053642273\n",
      "[step: 37] loss: 0.683884859085083\n",
      "[step: 38] loss: 0.6434528231620789\n",
      "[step: 39] loss: 0.652564287185669\n",
      "[step: 40] loss: 0.6134730577468872\n",
      "[step: 41] loss: 0.6439837217330933\n",
      "[step: 42] loss: 0.6286851763725281\n",
      "[step: 43] loss: 0.6454110145568848\n",
      "[step: 44] loss: 0.6270560026168823\n",
      "[step: 45] loss: 0.6114398241043091\n",
      "[step: 46] loss: 0.6124733686447144\n",
      "[step: 47] loss: 0.5558422803878784\n",
      "[step: 48] loss: 0.6140643358230591\n",
      "[step: 49] loss: 0.5991495847702026\n",
      "[step: 50] loss: 0.6022753715515137\n",
      "[step: 51] loss: 0.5498201847076416\n",
      "[step: 52] loss: 0.515658974647522\n",
      "[step: 53] loss: 0.510725736618042\n",
      "[step: 54] loss: 0.502678632736206\n",
      "[step: 55] loss: 0.4990388751029968\n",
      "[step: 56] loss: 0.43788689374923706\n",
      "[step: 57] loss: 0.45673584938049316\n",
      "[step: 58] loss: 0.43034127354621887\n",
      "[step: 59] loss: 0.455190509557724\n",
      "[step: 60] loss: 0.44701212644577026\n",
      "[step: 61] loss: 0.43879956007003784\n",
      "[step: 62] loss: 0.4274997115135193\n",
      "[step: 63] loss: 0.4577103853225708\n",
      "[step: 64] loss: 0.43762829899787903\n",
      "[step: 65] loss: 0.42452943325042725\n",
      "[step: 66] loss: 0.3988943099975586\n",
      "[step: 67] loss: 0.4147410988807678\n",
      "[step: 68] loss: 0.42551377415657043\n",
      "[step: 69] loss: 0.4212527275085449\n",
      "[step: 70] loss: 0.40942010283470154\n",
      "[step: 71] loss: 0.3773499131202698\n",
      "[step: 72] loss: 0.39232581853866577\n",
      "[step: 73] loss: 0.42036038637161255\n",
      "[step: 74] loss: 0.4038630425930023\n",
      "[step: 75] loss: 0.3952556252479553\n",
      "[step: 76] loss: 0.43367576599121094\n",
      "[step: 77] loss: 0.3675874173641205\n",
      "[step: 78] loss: 0.3972100615501404\n",
      "[step: 79] loss: 0.3845703601837158\n",
      "[step: 80] loss: 0.3681955933570862\n",
      "[step: 81] loss: 0.36077016592025757\n",
      "[step: 82] loss: 0.3425978422164917\n",
      "[step: 83] loss: 0.3344687819480896\n",
      "[step: 84] loss: 0.36264312267303467\n",
      "[step: 85] loss: 0.3883326053619385\n",
      "[step: 86] loss: 0.3787670135498047\n",
      "[step: 87] loss: 0.3544633984565735\n",
      "[step: 88] loss: 0.36932432651519775\n",
      "[step: 89] loss: 0.33752933144569397\n",
      "[step: 90] loss: 0.32162708044052124\n",
      "[step: 91] loss: 0.3496059775352478\n",
      "[step: 92] loss: 0.3663831949234009\n",
      "[step: 93] loss: 0.35498306155204773\n",
      "[step: 94] loss: 0.36822155117988586\n",
      "[step: 95] loss: 0.3350791335105896\n",
      "[step: 96] loss: 0.31936269998550415\n",
      "[step: 97] loss: 0.3326472342014313\n",
      "[step: 98] loss: 0.33556103706359863\n",
      "[step: 99] loss: 0.3595261573791504\n",
      "[step: 100] loss: 0.3337307870388031\n",
      "[step: 101] loss: 0.33601027727127075\n",
      "[step: 102] loss: 0.326171875\n",
      "[step: 103] loss: 0.3405981957912445\n",
      "[step: 104] loss: 0.37372344732284546\n",
      "[step: 105] loss: 0.3510860204696655\n",
      "[step: 106] loss: 0.3084459602832794\n",
      "[step: 107] loss: 0.3482094407081604\n",
      "[step: 108] loss: 0.33669260144233704\n",
      "[step: 109] loss: 0.3437274396419525\n",
      "[step: 110] loss: 0.34009838104248047\n",
      "[step: 111] loss: 0.36366936564445496\n",
      "[step: 112] loss: 0.33434537053108215\n",
      "[step: 113] loss: 0.295920729637146\n",
      "[step: 114] loss: 0.33206677436828613\n",
      "[step: 115] loss: 0.28997427225112915\n",
      "[step: 116] loss: 0.32187938690185547\n",
      "[step: 117] loss: 0.30256396532058716\n",
      "[step: 118] loss: 0.3329216241836548\n",
      "[step: 119] loss: 0.33537107706069946\n",
      "[step: 120] loss: 0.35260993242263794\n",
      "[step: 121] loss: 0.3077608644962311\n",
      "[step: 122] loss: 0.32707589864730835\n",
      "[step: 123] loss: 0.31020137667655945\n",
      "[step: 124] loss: 0.33532246947288513\n",
      "[step: 125] loss: 0.3324427604675293\n",
      "[step: 126] loss: 0.29223042726516724\n",
      "[step: 127] loss: 0.3225867450237274\n",
      "[step: 128] loss: 0.328339159488678\n",
      "[step: 129] loss: 0.32830438017845154\n",
      "[step: 130] loss: 0.30157583951950073\n",
      "[step: 131] loss: 0.3028123378753662\n",
      "[step: 132] loss: 0.31159186363220215\n",
      "[step: 133] loss: 0.33139094710350037\n",
      "[step: 134] loss: 0.33810895681381226\n",
      "[step: 135] loss: 0.29844963550567627\n",
      "[step: 136] loss: 0.30675220489501953\n",
      "[step: 137] loss: 0.2943376302719116\n",
      "[step: 138] loss: 0.313295841217041\n",
      "[step: 139] loss: 0.3111084997653961\n",
      "[step: 140] loss: 0.3066485524177551\n",
      "[step: 141] loss: 0.286283940076828\n",
      "[step: 142] loss: 0.3377845883369446\n",
      "[step: 143] loss: 0.2896810472011566\n",
      "[step: 144] loss: 0.3264540433883667\n",
      "[step: 145] loss: 0.30329328775405884\n",
      "[step: 146] loss: 0.3010702133178711\n",
      "[step: 147] loss: 0.3317626118659973\n",
      "[step: 148] loss: 0.3047798275947571\n",
      "[step: 149] loss: 0.3194020390510559\n",
      "[step: 150] loss: 0.28739693760871887\n",
      "[step: 151] loss: 0.31054747104644775\n",
      "[step: 152] loss: 0.3224334418773651\n",
      "[step: 153] loss: 0.33972522616386414\n",
      "[step: 154] loss: 0.29378676414489746\n",
      "[step: 155] loss: 0.34106260538101196\n",
      "[step: 156] loss: 0.30136311054229736\n",
      "[step: 157] loss: 0.32069671154022217\n",
      "[step: 158] loss: 0.3242432475090027\n",
      "[step: 159] loss: 0.3091846704483032\n",
      "[step: 160] loss: 0.30884766578674316\n",
      "[step: 161] loss: 0.30267083644866943\n",
      "[step: 162] loss: 0.27874383330345154\n",
      "[step: 163] loss: 0.2928519546985626\n",
      "[step: 164] loss: 0.3350324034690857\n",
      "[step: 165] loss: 0.32169151306152344\n",
      "[step: 166] loss: 0.3039233088493347\n",
      "[step: 167] loss: 0.32547587156295776\n",
      "[step: 168] loss: 0.2818998694419861\n",
      "[step: 169] loss: 0.27871575951576233\n",
      "[step: 170] loss: 0.3127230703830719\n",
      "[step: 171] loss: 0.3144819736480713\n",
      "[step: 172] loss: 0.3062817454338074\n",
      "[step: 173] loss: 0.3273060917854309\n",
      "[step: 174] loss: 0.2794596552848816\n",
      "[step: 175] loss: 0.2784714102745056\n",
      "[step: 176] loss: 0.28547951579093933\n",
      "[step: 177] loss: 0.29214420914649963\n",
      "[step: 178] loss: 0.3088495135307312\n",
      "[step: 179] loss: 0.30189383029937744\n",
      "[step: 180] loss: 0.28293943405151367\n",
      "[step: 181] loss: 0.2972378730773926\n",
      "[step: 182] loss: 0.3062897324562073\n",
      "[step: 183] loss: 0.3355275094509125\n",
      "[step: 184] loss: 0.32889068126678467\n",
      "[step: 185] loss: 0.2617979645729065\n",
      "[step: 186] loss: 0.33546507358551025\n",
      "[step: 187] loss: 0.31417855620384216\n",
      "[step: 188] loss: 0.29810982942581177\n",
      "[step: 189] loss: 0.32601726055145264\n",
      "[step: 190] loss: 0.3437119722366333\n",
      "[step: 191] loss: 0.28646549582481384\n",
      "[step: 192] loss: 0.27020370960235596\n",
      "[step: 193] loss: 0.2843649685382843\n",
      "[step: 194] loss: 0.26082801818847656\n",
      "[step: 195] loss: 0.27266067266464233\n",
      "[step: 196] loss: 0.2962242066860199\n",
      "[step: 197] loss: 0.2820674180984497\n",
      "[step: 198] loss: 0.2795374393463135\n",
      "[step: 199] loss: 0.28658604621887207\n",
      "[step: 200] loss: 0.25542759895324707\n",
      "[step: 201] loss: 0.24581021070480347\n",
      "[step: 202] loss: 0.24095764756202698\n",
      "[step: 203] loss: 0.2592138350009918\n",
      "[step: 204] loss: 0.24645587801933289\n",
      "[step: 205] loss: 0.21353408694267273\n",
      "[step: 206] loss: 0.20778809487819672\n",
      "[step: 207] loss: 0.20784452557563782\n",
      "[step: 208] loss: 0.20817017555236816\n",
      "[step: 209] loss: 0.18991179764270782\n",
      "[step: 210] loss: 0.18498530983924866\n",
      "[step: 211] loss: 0.18667422235012054\n",
      "[step: 212] loss: 0.17788654565811157\n",
      "[step: 213] loss: 0.22309711575508118\n",
      "[step: 214] loss: 0.17349842190742493\n",
      "[step: 215] loss: 0.20196376740932465\n",
      "[step: 216] loss: 0.20516961812973022\n",
      "[step: 217] loss: 0.21506330370903015\n",
      "[step: 218] loss: 0.1923949271440506\n",
      "[step: 219] loss: 0.17198336124420166\n",
      "[step: 220] loss: 0.18280304968357086\n",
      "[step: 221] loss: 0.2202981859445572\n",
      "[step: 222] loss: 0.18550032377243042\n",
      "[step: 223] loss: 0.20787504315376282\n",
      "[step: 224] loss: 0.1790785789489746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 225] loss: 0.20147070288658142\n",
      "[step: 226] loss: 0.1778988242149353\n",
      "[step: 227] loss: 0.1839306652545929\n",
      "[step: 228] loss: 0.18369171023368835\n",
      "[step: 229] loss: 0.14596998691558838\n",
      "[step: 230] loss: 0.15600723028182983\n",
      "[step: 231] loss: 0.18612441420555115\n",
      "[step: 232] loss: 0.18840959668159485\n",
      "[step: 233] loss: 0.1482822597026825\n",
      "[step: 234] loss: 0.17613324522972107\n",
      "[step: 235] loss: 0.1542210876941681\n",
      "[step: 236] loss: 0.1674022674560547\n",
      "[step: 237] loss: 0.17261892557144165\n",
      "[step: 238] loss: 0.15582507848739624\n",
      "[step: 239] loss: 0.1514674425125122\n",
      "[step: 240] loss: 0.1436653584241867\n",
      "[step: 241] loss: 0.14705637097358704\n",
      "[step: 242] loss: 0.15277187526226044\n",
      "[step: 243] loss: 0.17603152990341187\n",
      "[step: 244] loss: 0.1973428726196289\n",
      "[step: 245] loss: 0.21344521641731262\n",
      "[step: 246] loss: 0.1660567820072174\n",
      "[step: 247] loss: 0.15785732865333557\n",
      "[step: 248] loss: 0.1402185708284378\n",
      "[step: 249] loss: 0.17570924758911133\n",
      "[step: 250] loss: 0.15328462421894073\n",
      "[step: 251] loss: 0.14634957909584045\n",
      "[step: 252] loss: 0.14880627393722534\n",
      "[step: 253] loss: 0.1339544951915741\n",
      "[step: 254] loss: 0.15331363677978516\n",
      "[step: 255] loss: 0.14732098579406738\n",
      "[step: 256] loss: 0.13963842391967773\n",
      "[step: 257] loss: 0.14044690132141113\n",
      "[step: 258] loss: 0.13832831382751465\n",
      "[step: 259] loss: 0.14734548330307007\n",
      "[step: 260] loss: 0.14885900914669037\n",
      "[step: 261] loss: 0.2825204133987427\n",
      "[step: 262] loss: 0.18508939445018768\n",
      "[step: 263] loss: 0.1876378357410431\n",
      "[step: 264] loss: 0.17391814291477203\n",
      "[step: 265] loss: 0.2213623821735382\n",
      "[step: 266] loss: 0.18113699555397034\n",
      "[step: 267] loss: 0.19146712124347687\n",
      "[step: 268] loss: 0.20826053619384766\n",
      "[step: 269] loss: 0.24821896851062775\n",
      "[step: 270] loss: 0.15227077901363373\n",
      "[step: 271] loss: 0.19453836977481842\n",
      "[step: 272] loss: 0.26735618710517883\n",
      "[step: 273] loss: 0.2530547082424164\n",
      "[step: 274] loss: 0.18895694613456726\n",
      "[step: 275] loss: 0.25074294209480286\n",
      "[step: 276] loss: 0.2123377025127411\n",
      "[step: 277] loss: 0.21007132530212402\n",
      "[step: 278] loss: 0.23492512106895447\n",
      "[step: 279] loss: 0.17892462015151978\n",
      "[step: 280] loss: 0.17196446657180786\n",
      "[step: 281] loss: 0.1987583041191101\n",
      "[step: 282] loss: 0.16865941882133484\n",
      "[step: 283] loss: 0.155047208070755\n",
      "[step: 284] loss: 0.2081020176410675\n",
      "[step: 285] loss: 0.13296589255332947\n",
      "[step: 286] loss: 0.18822234869003296\n",
      "[step: 287] loss: 0.18116046488285065\n",
      "[step: 288] loss: 0.14214998483657837\n",
      "[step: 289] loss: 0.15463975071907043\n",
      "[step: 290] loss: 0.1675654649734497\n",
      "[step: 291] loss: 0.17605583369731903\n",
      "[step: 292] loss: 0.19229987263679504\n",
      "[step: 293] loss: 0.1526881456375122\n",
      "[step: 294] loss: 0.16259025037288666\n",
      "[step: 295] loss: 0.12424825131893158\n",
      "[step: 296] loss: 0.14617866277694702\n",
      "[step: 297] loss: 0.1868148148059845\n",
      "[step: 298] loss: 0.1563975214958191\n",
      "[step: 299] loss: 0.13154810667037964\n",
      "[step: 300] loss: 0.15708446502685547\n",
      "[step: 301] loss: 0.1594850718975067\n",
      "[step: 302] loss: 0.16672924160957336\n",
      "[step: 303] loss: 0.12572918832302094\n",
      "[step: 304] loss: 0.16431963443756104\n",
      "[step: 305] loss: 0.13051371276378632\n",
      "[step: 306] loss: 0.16293659806251526\n",
      "[step: 307] loss: 0.14907273650169373\n",
      "[step: 308] loss: 0.13672508299350739\n",
      "[step: 309] loss: 0.12934616208076477\n",
      "[step: 310] loss: 0.15052086114883423\n",
      "[step: 311] loss: 0.17715951800346375\n",
      "[step: 312] loss: 0.12042060494422913\n",
      "[step: 313] loss: 0.14333339035511017\n",
      "[step: 314] loss: 0.1602742075920105\n",
      "[step: 315] loss: 0.1365056037902832\n",
      "[step: 316] loss: 0.13237309455871582\n",
      "[step: 317] loss: 0.13245169818401337\n",
      "[step: 318] loss: 0.12600994110107422\n",
      "[step: 319] loss: 0.1275322139263153\n",
      "[step: 320] loss: 0.11616945266723633\n",
      "[step: 321] loss: 0.1516937017440796\n",
      "[step: 322] loss: 0.1419529914855957\n",
      "[step: 323] loss: 0.12901350855827332\n",
      "[step: 324] loss: 0.16960637271404266\n",
      "[step: 325] loss: 0.15406331419944763\n",
      "[step: 326] loss: 0.1490856111049652\n",
      "[step: 327] loss: 0.12930545210838318\n",
      "[step: 328] loss: 0.13592380285263062\n",
      "[step: 329] loss: 0.1435011774301529\n",
      "[step: 330] loss: 0.14970049262046814\n",
      "[step: 331] loss: 0.1338416486978531\n",
      "[step: 332] loss: 0.12610521912574768\n",
      "[step: 333] loss: 0.12669996917247772\n",
      "[step: 334] loss: 0.11558661609888077\n",
      "[step: 335] loss: 0.1360517293214798\n",
      "[step: 336] loss: 0.12323563545942307\n",
      "[step: 337] loss: 0.128651425242424\n",
      "[step: 338] loss: 0.10516507923603058\n",
      "[step: 339] loss: 0.11897435784339905\n",
      "[step: 340] loss: 0.15930397808551788\n",
      "[step: 341] loss: 0.12478019297122955\n",
      "[step: 342] loss: 0.12532877922058105\n",
      "[step: 343] loss: 0.12125734984874725\n",
      "[step: 344] loss: 0.12810760736465454\n",
      "[step: 345] loss: 0.12877777218818665\n",
      "[step: 346] loss: 0.12642823159694672\n",
      "[step: 347] loss: 0.14636841416358948\n",
      "[step: 348] loss: 0.126291424036026\n",
      "[step: 349] loss: 0.11644968390464783\n",
      "[step: 350] loss: 0.099286288022995\n",
      "[step: 351] loss: 0.11126147210597992\n",
      "[step: 352] loss: 0.10715267062187195\n",
      "[step: 353] loss: 0.09374275803565979\n",
      "[step: 354] loss: 0.11249235272407532\n",
      "[step: 355] loss: 0.11899908632040024\n",
      "[step: 356] loss: 0.14686104655265808\n",
      "[step: 357] loss: 0.12519225478172302\n",
      "[step: 358] loss: 0.11684022843837738\n",
      "[step: 359] loss: 0.09367674589157104\n",
      "[step: 360] loss: 0.09390708804130554\n",
      "[step: 361] loss: 0.10788740962743759\n",
      "[step: 362] loss: 0.09888273477554321\n",
      "[step: 363] loss: 0.13187874853610992\n",
      "[step: 364] loss: 0.09231538325548172\n",
      "[step: 365] loss: 0.1079052984714508\n",
      "[step: 366] loss: 0.10652413964271545\n",
      "[step: 367] loss: 0.11514126509428024\n",
      "[step: 368] loss: 0.10035374760627747\n",
      "[step: 369] loss: 0.09465786814689636\n",
      "[step: 370] loss: 0.10672521591186523\n",
      "[step: 371] loss: 0.12669241428375244\n",
      "[step: 372] loss: 0.09620775282382965\n",
      "[step: 373] loss: 0.11035346984863281\n",
      "[step: 374] loss: 0.08620118349790573\n",
      "[step: 375] loss: 0.09016764163970947\n",
      "[step: 376] loss: 0.12684381008148193\n",
      "[step: 377] loss: 0.11986386775970459\n",
      "[step: 378] loss: 0.09297925978899002\n",
      "[step: 379] loss: 0.10027647018432617\n",
      "[step: 380] loss: 0.09736606478691101\n",
      "[step: 381] loss: 0.11739251017570496\n",
      "[step: 382] loss: 0.10868670791387558\n",
      "[step: 383] loss: 0.11121858656406403\n",
      "[step: 384] loss: 0.09727120399475098\n",
      "[step: 385] loss: 0.1069648489356041\n",
      "[step: 386] loss: 0.1015322357416153\n",
      "[step: 387] loss: 0.1170993447303772\n",
      "[step: 388] loss: 0.09318220615386963\n",
      "[step: 389] loss: 0.10125328600406647\n",
      "[step: 390] loss: 0.12891367077827454\n",
      "[step: 391] loss: 0.09414968639612198\n",
      "[step: 392] loss: 0.09882913529872894\n",
      "[step: 393] loss: 0.11287622898817062\n",
      "[step: 394] loss: 0.11297740787267685\n",
      "[step: 395] loss: 0.11461816728115082\n",
      "[step: 396] loss: 0.10945743322372437\n",
      "[step: 397] loss: 0.08194465935230255\n",
      "[step: 398] loss: 0.09433948993682861\n",
      "[step: 399] loss: 0.07964886724948883\n",
      "[step: 400] loss: 0.11462624371051788\n",
      "[step: 401] loss: 0.10364765673875809\n",
      "[step: 402] loss: 0.09312543272972107\n",
      "[step: 403] loss: 0.13719236850738525\n",
      "[step: 404] loss: 0.11993038654327393\n",
      "[step: 405] loss: 0.10049170255661011\n",
      "[step: 406] loss: 0.1168830618262291\n",
      "[step: 407] loss: 0.1044483333826065\n",
      "[step: 408] loss: 0.10644660890102386\n",
      "[step: 409] loss: 0.11458154022693634\n",
      "[step: 410] loss: 0.0893496721982956\n",
      "[step: 411] loss: 0.10671073943376541\n",
      "[step: 412] loss: 0.0929209440946579\n",
      "[step: 413] loss: 0.12490974366664886\n",
      "[step: 414] loss: 0.09684485197067261\n",
      "[step: 415] loss: 0.08938582986593246\n",
      "[step: 416] loss: 0.10163372755050659\n",
      "[step: 417] loss: 0.0791248232126236\n",
      "[step: 418] loss: 0.10064287483692169\n",
      "[step: 419] loss: 0.11301910877227783\n",
      "[step: 420] loss: 0.12712740898132324\n",
      "[step: 421] loss: 0.1164616197347641\n",
      "[step: 422] loss: 0.09620386362075806\n",
      "[step: 423] loss: 0.09183001518249512\n",
      "[step: 424] loss: 0.11046174168586731\n",
      "[step: 425] loss: 0.10306072235107422\n",
      "[step: 426] loss: 0.122206911444664\n",
      "[step: 427] loss: 0.11327174305915833\n",
      "[step: 428] loss: 0.10909052938222885\n",
      "[step: 429] loss: 0.09439240396022797\n",
      "[step: 430] loss: 0.08754275739192963\n",
      "[step: 431] loss: 0.08594053238630295\n",
      "[step: 432] loss: 0.09397144615650177\n",
      "[step: 433] loss: 0.11732049286365509\n",
      "[step: 434] loss: 0.0984070748090744\n",
      "[step: 435] loss: 0.13544195890426636\n",
      "[step: 436] loss: 0.09939420223236084\n",
      "[step: 437] loss: 0.10624073445796967\n",
      "[step: 438] loss: 0.08478604257106781\n",
      "[step: 439] loss: 0.08402315527200699\n",
      "[step: 440] loss: 0.08821873366832733\n",
      "[step: 441] loss: 0.0998172014951706\n",
      "[step: 442] loss: 0.12012727558612823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 443] loss: 0.08363862335681915\n",
      "[step: 444] loss: 0.09026548266410828\n",
      "[step: 445] loss: 0.09487493336200714\n",
      "[step: 446] loss: 0.11362206935882568\n",
      "[step: 447] loss: 0.07685619592666626\n",
      "[step: 448] loss: 0.08694148063659668\n",
      "[step: 449] loss: 0.08828479051589966\n",
      "[step: 450] loss: 0.12654830515384674\n",
      "[step: 451] loss: 0.08394722640514374\n",
      "[step: 452] loss: 0.09362143278121948\n",
      "[step: 453] loss: 0.0816316232085228\n",
      "[step: 454] loss: 0.07405291497707367\n",
      "[step: 455] loss: 0.09716018289327621\n",
      "[step: 456] loss: 0.09729664772748947\n",
      "[step: 457] loss: 0.08638593554496765\n",
      "[step: 458] loss: 0.08077973127365112\n",
      "[step: 459] loss: 0.09721516817808151\n",
      "[step: 460] loss: 0.09908752143383026\n",
      "[step: 461] loss: 0.09547989070415497\n",
      "[step: 462] loss: 0.09384915232658386\n",
      "[step: 463] loss: 0.08489514887332916\n",
      "[step: 464] loss: 0.10253145545721054\n",
      "[step: 465] loss: 0.0808345302939415\n",
      "[step: 466] loss: 0.09908749908208847\n",
      "[step: 467] loss: 0.08394177258014679\n",
      "[step: 468] loss: 0.0939890593290329\n",
      "[step: 469] loss: 0.10711787641048431\n",
      "[step: 470] loss: 0.09098844975233078\n",
      "[step: 471] loss: 0.0920555591583252\n",
      "[step: 472] loss: 0.1083468347787857\n",
      "[step: 473] loss: 0.09854816645383835\n",
      "[step: 474] loss: 0.10123269259929657\n",
      "[step: 475] loss: 0.09080787003040314\n",
      "[step: 476] loss: 0.08868825435638428\n",
      "[step: 477] loss: 0.0887378379702568\n",
      "[step: 478] loss: 0.08366682380437851\n",
      "[step: 479] loss: 0.09044250100851059\n",
      "[step: 480] loss: 0.0909339189529419\n",
      "[step: 481] loss: 0.09068448841571808\n",
      "[step: 482] loss: 0.13252845406532288\n",
      "[step: 483] loss: 0.11228958517313004\n",
      "[step: 484] loss: 0.09387312829494476\n",
      "[step: 485] loss: 0.09561751782894135\n",
      "[step: 486] loss: 0.0838920846581459\n",
      "[step: 487] loss: 0.08351844549179077\n",
      "[step: 488] loss: 0.09092330187559128\n",
      "[step: 489] loss: 0.07601422816514969\n",
      "[step: 490] loss: 0.08959558606147766\n",
      "[step: 491] loss: 0.08283118903636932\n",
      "[step: 492] loss: 0.09424161165952682\n",
      "[step: 493] loss: 0.08579613268375397\n",
      "[step: 494] loss: 0.07520264387130737\n",
      "[step: 495] loss: 0.09281317889690399\n",
      "[step: 496] loss: 0.06491684913635254\n",
      "[step: 497] loss: 0.07826921343803406\n",
      "[step: 498] loss: 0.09154772758483887\n",
      "[step: 499] loss: 0.10561364889144897\n",
      "Test accuracy: 0.730\n",
      "[step: 0] loss: 0.0987343043088913\n",
      "[step: 1] loss: 0.07845336198806763\n",
      "[step: 2] loss: 0.07174031436443329\n",
      "[step: 3] loss: 0.09265775978565216\n",
      "[step: 4] loss: 0.09811268746852875\n",
      "[step: 5] loss: 0.10773639380931854\n",
      "[step: 6] loss: 0.06998607516288757\n",
      "[step: 7] loss: 0.09820130467414856\n",
      "[step: 8] loss: 0.07828779518604279\n",
      "[step: 9] loss: 0.07900019735097885\n",
      "[step: 10] loss: 0.0845336765050888\n",
      "[step: 11] loss: 0.06929818540811539\n",
      "[step: 12] loss: 0.09662425518035889\n",
      "[step: 13] loss: 0.09668129682540894\n",
      "[step: 14] loss: 0.0991707369685173\n",
      "[step: 15] loss: 0.0793253555893898\n",
      "[step: 16] loss: 0.08735264837741852\n",
      "[step: 17] loss: 0.06741213798522949\n",
      "[step: 18] loss: 0.06573823094367981\n",
      "[step: 19] loss: 0.06934604793787003\n",
      "[step: 20] loss: 0.08280555158853531\n",
      "[step: 21] loss: 0.09198212623596191\n",
      "[step: 22] loss: 0.07245030999183655\n",
      "[step: 23] loss: 0.08252158761024475\n",
      "[step: 24] loss: 0.06775671243667603\n",
      "[step: 25] loss: 0.08936724066734314\n",
      "[step: 26] loss: 0.07018066197633743\n",
      "[step: 27] loss: 0.07370773702859879\n",
      "[step: 28] loss: 0.0756797194480896\n",
      "[step: 29] loss: 0.0857761800289154\n",
      "[step: 30] loss: 0.0763256624341011\n",
      "[step: 31] loss: 0.068197101354599\n",
      "[step: 32] loss: 0.07708540558815002\n",
      "[step: 33] loss: 0.06157569959759712\n",
      "[step: 34] loss: 0.07841882109642029\n",
      "[step: 35] loss: 0.08404377847909927\n",
      "[step: 36] loss: 0.07035824656486511\n",
      "[step: 37] loss: 0.06871505081653595\n",
      "[step: 38] loss: 0.08896887302398682\n",
      "[step: 39] loss: 0.0842270702123642\n",
      "[step: 40] loss: 0.06978738307952881\n",
      "[step: 41] loss: 0.08062127232551575\n",
      "[step: 42] loss: 0.07429970055818558\n",
      "[step: 43] loss: 0.08484410494565964\n",
      "[step: 44] loss: 0.076870396733284\n",
      "[step: 45] loss: 0.08338463306427002\n",
      "[step: 46] loss: 0.07455817610025406\n",
      "[step: 47] loss: 0.08081988990306854\n",
      "[step: 48] loss: 0.09776560962200165\n",
      "[step: 49] loss: 0.0773603692650795\n",
      "[step: 50] loss: 0.07455587387084961\n",
      "[step: 51] loss: 0.09741196781396866\n",
      "[step: 52] loss: 0.0772845521569252\n",
      "[step: 53] loss: 0.07654682546854019\n",
      "[step: 54] loss: 0.08719976246356964\n",
      "[step: 55] loss: 0.07992768287658691\n",
      "[step: 56] loss: 0.0697321891784668\n",
      "[step: 57] loss: 0.07570755481719971\n",
      "[step: 58] loss: 0.08266784995794296\n",
      "[step: 59] loss: 0.08264307677745819\n",
      "[step: 60] loss: 0.0832766592502594\n",
      "[step: 61] loss: 0.09452486783266068\n",
      "[step: 62] loss: 0.094990074634552\n",
      "[step: 63] loss: 0.09728667140007019\n",
      "[step: 64] loss: 0.08280743658542633\n",
      "[step: 65] loss: 0.07309899479150772\n",
      "[step: 66] loss: 0.07616648823022842\n",
      "[step: 67] loss: 0.07622569054365158\n",
      "[step: 68] loss: 0.07099048793315887\n",
      "[step: 69] loss: 0.0792308896780014\n",
      "[step: 70] loss: 0.08324223756790161\n",
      "[step: 71] loss: 0.07624448835849762\n",
      "[step: 72] loss: 0.0681830570101738\n",
      "[step: 73] loss: 0.08160693198442459\n",
      "[step: 74] loss: 0.0859350860118866\n",
      "[step: 75] loss: 0.06634208559989929\n",
      "[step: 76] loss: 0.07291220128536224\n",
      "[step: 77] loss: 0.09139242023229599\n",
      "[step: 78] loss: 0.08914970606565475\n",
      "[step: 79] loss: 0.11066706478595734\n",
      "[step: 80] loss: 0.07874831557273865\n",
      "[step: 81] loss: 0.07346245646476746\n",
      "[step: 82] loss: 0.0826582983136177\n",
      "[step: 83] loss: 0.09493821859359741\n",
      "[step: 84] loss: 0.09936758130788803\n",
      "[step: 85] loss: 0.06939220428466797\n",
      "[step: 86] loss: 0.10426206141710281\n",
      "[step: 87] loss: 0.07571804523468018\n",
      "[step: 88] loss: 0.06640614569187164\n",
      "[step: 89] loss: 0.09128162264823914\n",
      "[step: 90] loss: 0.05965494364500046\n",
      "[step: 91] loss: 0.09756159037351608\n",
      "[step: 92] loss: 0.08367833495140076\n",
      "[step: 93] loss: 0.09982167184352875\n",
      "[step: 94] loss: 0.08339940011501312\n",
      "[step: 95] loss: 0.09082284569740295\n",
      "[step: 96] loss: 0.05820399150252342\n",
      "[step: 97] loss: 0.06191467493772507\n",
      "[step: 98] loss: 0.06680814921855927\n",
      "[step: 99] loss: 0.07917457818984985\n",
      "[step: 100] loss: 0.08095191419124603\n",
      "[step: 101] loss: 0.08617539703845978\n",
      "[step: 102] loss: 0.07895037531852722\n",
      "[step: 103] loss: 0.06057263910770416\n",
      "[step: 104] loss: 0.0886869803071022\n",
      "[step: 105] loss: 0.06732858717441559\n",
      "[step: 106] loss: 0.052036408334970474\n",
      "[step: 107] loss: 0.07804641872644424\n",
      "[step: 108] loss: 0.09464266151189804\n",
      "[step: 109] loss: 0.06973914802074432\n",
      "[step: 110] loss: 0.061971958726644516\n",
      "[step: 111] loss: 0.06976345926523209\n",
      "[step: 112] loss: 0.0633738562464714\n",
      "[step: 113] loss: 0.07523974776268005\n",
      "[step: 114] loss: 0.07867152243852615\n",
      "[step: 115] loss: 0.0696553885936737\n",
      "[step: 116] loss: 0.06497950851917267\n",
      "[step: 117] loss: 0.0855071097612381\n",
      "[step: 118] loss: 0.07226738333702087\n",
      "[step: 119] loss: 0.07207255810499191\n",
      "[step: 120] loss: 0.07624346017837524\n",
      "[step: 121] loss: 0.07903018593788147\n",
      "[step: 122] loss: 0.08140724152326584\n",
      "[step: 123] loss: 0.0663214698433876\n",
      "[step: 124] loss: 0.08671633154153824\n",
      "[step: 125] loss: 0.07804542034864426\n",
      "[step: 126] loss: 0.08061861991882324\n",
      "[step: 127] loss: 0.08973148465156555\n",
      "[step: 128] loss: 0.07272139191627502\n",
      "[step: 129] loss: 0.0819377452135086\n",
      "[step: 130] loss: 0.07578529417514801\n",
      "[step: 131] loss: 0.0838363841176033\n",
      "[step: 132] loss: 0.06776734441518784\n",
      "[step: 133] loss: 0.08312799036502838\n",
      "[step: 134] loss: 0.08589214831590652\n",
      "[step: 135] loss: 0.06146346777677536\n",
      "[step: 136] loss: 0.0649213194847107\n",
      "[step: 137] loss: 0.07305692136287689\n",
      "[step: 138] loss: 0.08564230799674988\n",
      "[step: 139] loss: 0.0801822766661644\n",
      "[step: 140] loss: 0.09272371232509613\n",
      "[step: 141] loss: 0.08139178156852722\n",
      "[step: 142] loss: 0.0952359139919281\n",
      "[step: 143] loss: 0.08361935615539551\n",
      "[step: 144] loss: 0.061216749250888824\n",
      "[step: 145] loss: 0.08185899257659912\n",
      "[step: 146] loss: 0.07524670660495758\n",
      "[step: 147] loss: 0.07347982376813889\n",
      "[step: 148] loss: 0.0795118659734726\n",
      "[step: 149] loss: 0.09317761659622192\n",
      "[step: 150] loss: 0.0762186348438263\n",
      "[step: 151] loss: 0.054322049021720886\n",
      "[step: 152] loss: 0.07335619628429413\n",
      "[step: 153] loss: 0.09420212358236313\n",
      "[step: 154] loss: 0.06291178613901138\n",
      "[step: 155] loss: 0.06184495985507965\n",
      "[step: 156] loss: 0.06965233385562897\n",
      "[step: 157] loss: 0.08819413930177689\n",
      "[step: 158] loss: 0.11105633527040482\n",
      "[step: 159] loss: 0.0763716995716095\n",
      "[step: 160] loss: 0.07565565407276154\n",
      "[step: 161] loss: 0.08238059282302856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 162] loss: 0.08457586169242859\n",
      "[step: 163] loss: 0.09090946614742279\n",
      "[step: 164] loss: 0.07629240304231644\n",
      "[step: 165] loss: 0.0921359583735466\n",
      "[step: 166] loss: 0.07702124118804932\n",
      "[step: 167] loss: 0.05549941584467888\n",
      "[step: 168] loss: 0.09109747409820557\n",
      "[step: 169] loss: 0.05176059156656265\n",
      "[step: 170] loss: 0.08818794786930084\n",
      "[step: 171] loss: 0.08006605505943298\n",
      "[step: 172] loss: 0.09875117242336273\n",
      "[step: 173] loss: 0.07620944082736969\n",
      "[step: 174] loss: 0.08063238114118576\n",
      "[step: 175] loss: 0.06361838430166245\n",
      "[step: 176] loss: 0.05140473693609238\n",
      "[step: 177] loss: 0.05960940942168236\n",
      "[step: 178] loss: 0.07221539318561554\n",
      "[step: 179] loss: 0.07814033329486847\n",
      "[step: 180] loss: 0.07910725474357605\n",
      "[step: 181] loss: 0.06933646649122238\n",
      "[step: 182] loss: 0.06007182225584984\n",
      "[step: 183] loss: 0.07715068757534027\n",
      "[step: 184] loss: 0.0808628648519516\n",
      "[step: 185] loss: 0.04509631544351578\n",
      "[step: 186] loss: 0.08483563363552094\n",
      "[step: 187] loss: 0.07801759243011475\n",
      "[step: 188] loss: 0.07358160614967346\n",
      "[step: 189] loss: 0.06348244845867157\n",
      "[step: 190] loss: 0.07162155956029892\n",
      "[step: 191] loss: 0.05925466865301132\n",
      "[step: 192] loss: 0.07086265087127686\n",
      "[step: 193] loss: 0.08539273589849472\n",
      "[step: 194] loss: 0.07426454871892929\n",
      "[step: 195] loss: 0.06379242241382599\n",
      "[step: 196] loss: 0.07391504943370819\n",
      "[step: 197] loss: 0.08429653942584991\n",
      "[step: 198] loss: 0.07144956290721893\n",
      "[step: 199] loss: 0.06337139755487442\n",
      "[step: 200] loss: 0.07802478969097137\n",
      "[step: 201] loss: 0.07191155850887299\n",
      "[step: 202] loss: 0.06350281834602356\n",
      "[step: 203] loss: 0.07570424675941467\n",
      "[step: 204] loss: 0.06992209702730179\n",
      "[step: 205] loss: 0.07843794673681259\n",
      "[step: 206] loss: 0.07658948749303818\n",
      "[step: 207] loss: 0.08151096105575562\n",
      "[step: 208] loss: 0.06707018613815308\n",
      "[step: 209] loss: 0.08751469850540161\n",
      "[step: 210] loss: 0.08420318365097046\n",
      "[step: 211] loss: 0.06923893839120865\n",
      "[step: 212] loss: 0.08394023776054382\n",
      "[step: 213] loss: 0.07948528975248337\n",
      "[step: 214] loss: 0.05631241574883461\n",
      "[step: 215] loss: 0.06300346553325653\n",
      "[step: 216] loss: 0.06980305165052414\n",
      "[step: 217] loss: 0.07875074446201324\n",
      "[step: 218] loss: 0.08296613395214081\n",
      "[step: 219] loss: 0.09903765469789505\n",
      "[step: 220] loss: 0.08193690329790115\n",
      "[step: 221] loss: 0.09256677329540253\n",
      "[step: 222] loss: 0.0977715253829956\n",
      "[step: 223] loss: 0.04685430973768234\n",
      "[step: 224] loss: 0.08125521242618561\n",
      "[step: 225] loss: 0.07436535507440567\n",
      "[step: 226] loss: 0.07832855731248856\n",
      "[step: 227] loss: 0.07928062975406647\n",
      "[step: 228] loss: 0.0706263929605484\n",
      "[step: 229] loss: 0.06812629103660583\n",
      "[step: 230] loss: 0.06758420169353485\n",
      "[step: 231] loss: 0.07318617403507233\n",
      "[step: 232] loss: 0.08415862917900085\n",
      "[step: 233] loss: 0.06270252168178558\n",
      "[step: 234] loss: 0.05504370480775833\n",
      "[step: 235] loss: 0.06341471523046494\n",
      "[step: 236] loss: 0.10228363424539566\n",
      "[step: 237] loss: 0.09637817740440369\n",
      "[step: 238] loss: 0.07536634802818298\n",
      "[step: 239] loss: 0.06348586082458496\n",
      "[step: 240] loss: 0.08108788728713989\n",
      "[step: 241] loss: 0.07380754500627518\n",
      "[step: 242] loss: 0.0819292962551117\n",
      "[step: 243] loss: 0.08172394335269928\n",
      "[step: 244] loss: 0.10058091580867767\n",
      "[step: 245] loss: 0.06958627700805664\n",
      "[step: 246] loss: 0.06230491027235985\n",
      "[step: 247] loss: 0.08711346238851547\n",
      "[step: 248] loss: 0.05742788314819336\n",
      "[step: 249] loss: 0.0845990777015686\n",
      "[step: 250] loss: 0.08676258474588394\n",
      "[step: 251] loss: 0.09522359818220139\n",
      "[step: 252] loss: 0.0626160055398941\n",
      "[step: 253] loss: 0.08018520474433899\n",
      "[step: 254] loss: 0.07080140709877014\n",
      "[step: 255] loss: 0.05158357322216034\n",
      "[step: 256] loss: 0.05399428308010101\n",
      "[step: 257] loss: 0.06610140204429626\n",
      "[step: 258] loss: 0.07952353358268738\n",
      "[step: 259] loss: 0.07254128158092499\n",
      "[step: 260] loss: 0.06897484511137009\n",
      "[step: 261] loss: 0.060974035412073135\n",
      "[step: 262] loss: 0.06932435184717178\n",
      "[step: 263] loss: 0.08020631223917007\n",
      "[step: 264] loss: 0.04690873250365257\n",
      "[step: 265] loss: 0.06707561016082764\n",
      "[step: 266] loss: 0.06901717185974121\n",
      "[step: 267] loss: 0.07968062162399292\n",
      "[step: 268] loss: 0.058327045291662216\n",
      "[step: 269] loss: 0.07152757048606873\n",
      "[step: 270] loss: 0.05662086233496666\n",
      "[step: 271] loss: 0.06116088107228279\n",
      "[step: 272] loss: 0.0761590227484703\n",
      "[step: 273] loss: 0.07262073457241058\n",
      "[step: 274] loss: 0.06019500270485878\n",
      "[step: 275] loss: 0.07264527678489685\n",
      "[step: 276] loss: 0.0766308531165123\n",
      "[step: 277] loss: 0.07889475673437119\n",
      "[step: 278] loss: 0.060812607407569885\n",
      "[step: 279] loss: 0.07596049457788467\n",
      "[step: 280] loss: 0.06785466521978378\n",
      "[step: 281] loss: 0.060329750180244446\n",
      "[step: 282] loss: 0.07608333230018616\n",
      "[step: 283] loss: 0.07044865190982819\n",
      "[step: 284] loss: 0.06607583165168762\n",
      "[step: 285] loss: 0.07466353476047516\n",
      "[step: 286] loss: 0.09448395669460297\n",
      "[step: 287] loss: 0.0611877366900444\n",
      "[step: 288] loss: 0.08016213774681091\n",
      "[step: 289] loss: 0.08068795502185822\n",
      "[step: 290] loss: 0.05674063414335251\n",
      "[step: 291] loss: 0.07348072528839111\n",
      "[step: 292] loss: 0.07101696729660034\n",
      "[step: 293] loss: 0.06393468379974365\n",
      "[step: 294] loss: 0.052396878600120544\n",
      "[step: 295] loss: 0.06823469698429108\n",
      "[step: 296] loss: 0.07357791066169739\n",
      "[step: 297] loss: 0.07801800966262817\n",
      "[step: 298] loss: 0.07108552008867264\n",
      "[step: 299] loss: 0.08093085885047913\n",
      "[step: 300] loss: 0.0849459171295166\n",
      "[step: 301] loss: 0.0808841660618782\n",
      "[step: 302] loss: 0.052160266786813736\n",
      "[step: 303] loss: 0.08341701328754425\n",
      "[step: 304] loss: 0.06807568669319153\n",
      "[step: 305] loss: 0.06303102523088455\n",
      "[step: 306] loss: 0.062443241477012634\n",
      "[step: 307] loss: 0.07145944982767105\n",
      "[step: 308] loss: 0.06046398729085922\n",
      "[step: 309] loss: 0.06733164191246033\n",
      "[step: 310] loss: 0.061801835894584656\n",
      "[step: 311] loss: 0.07426552474498749\n",
      "[step: 312] loss: 0.057786405086517334\n",
      "[step: 313] loss: 0.05351809784770012\n",
      "[step: 314] loss: 0.05541758984327316\n",
      "[step: 315] loss: 0.08791908621788025\n",
      "[step: 316] loss: 0.08795760571956635\n",
      "[step: 317] loss: 0.07810620218515396\n",
      "[step: 318] loss: 0.06728609651327133\n",
      "[step: 319] loss: 0.0614050067961216\n",
      "[step: 320] loss: 0.07678785920143127\n",
      "[step: 321] loss: 0.06383860111236572\n",
      "[step: 322] loss: 0.07900628447532654\n",
      "[step: 323] loss: 0.09589647501707077\n",
      "[step: 324] loss: 0.054415419697761536\n",
      "[step: 325] loss: 0.05826817452907562\n",
      "[step: 326] loss: 0.08785401284694672\n",
      "[step: 327] loss: 0.06425473839044571\n",
      "[step: 328] loss: 0.0713571235537529\n",
      "[step: 329] loss: 0.06776155531406403\n",
      "[step: 330] loss: 0.08173392713069916\n",
      "[step: 331] loss: 0.07952423393726349\n",
      "[step: 332] loss: 0.07333716750144958\n",
      "[step: 333] loss: 0.07041364163160324\n",
      "[step: 334] loss: 0.04439046233892441\n",
      "[step: 335] loss: 0.05114166438579559\n",
      "[step: 336] loss: 0.05764275789260864\n",
      "[step: 337] loss: 0.0609915554523468\n",
      "[step: 338] loss: 0.08057162165641785\n",
      "[step: 339] loss: 0.0634421557188034\n",
      "[step: 340] loss: 0.05609559640288353\n",
      "[step: 341] loss: 0.05659458041191101\n",
      "[step: 342] loss: 0.07341744005680084\n",
      "[step: 343] loss: 0.051407426595687866\n",
      "[step: 344] loss: 0.06615501642227173\n",
      "[step: 345] loss: 0.06689195334911346\n",
      "[step: 346] loss: 0.07686735689640045\n",
      "[step: 347] loss: 0.049114301800727844\n",
      "[step: 348] loss: 0.06352052092552185\n",
      "[step: 349] loss: 0.048082366585731506\n",
      "[step: 350] loss: 0.06031340733170509\n",
      "[step: 351] loss: 0.06990653276443481\n",
      "[step: 352] loss: 0.0733194649219513\n",
      "[step: 353] loss: 0.06255590915679932\n",
      "[step: 354] loss: 0.07470257580280304\n",
      "[step: 355] loss: 0.07675284147262573\n",
      "[step: 356] loss: 0.06792715936899185\n",
      "[step: 357] loss: 0.07210197299718857\n",
      "[step: 358] loss: 0.0764923244714737\n",
      "[step: 359] loss: 0.06299137324094772\n",
      "[step: 360] loss: 0.06336764246225357\n",
      "[step: 361] loss: 0.06841056048870087\n",
      "[step: 362] loss: 0.06076566129922867\n",
      "[step: 363] loss: 0.07053904235363007\n",
      "[step: 364] loss: 0.07075722515583038\n",
      "[step: 365] loss: 0.09397467970848083\n",
      "[step: 366] loss: 0.057517267763614655\n",
      "[step: 367] loss: 0.07764450460672379\n",
      "[step: 368] loss: 0.08789893984794617\n",
      "[step: 369] loss: 0.055315010249614716\n",
      "[step: 370] loss: 0.06259763985872269\n",
      "[step: 371] loss: 0.07541671395301819\n",
      "[step: 372] loss: 0.05645020306110382\n",
      "[step: 373] loss: 0.0534219965338707\n",
      "[step: 374] loss: 0.06206078827381134\n",
      "[step: 375] loss: 0.07566669583320618\n",
      "[step: 376] loss: 0.07106326520442963\n",
      "[step: 377] loss: 0.07082723081111908\n",
      "[step: 378] loss: 0.09900471568107605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 379] loss: 0.07309561967849731\n",
      "[step: 380] loss: 0.07278093695640564\n",
      "[step: 381] loss: 0.06493207812309265\n",
      "[step: 382] loss: 0.07150764763355255\n",
      "[step: 383] loss: 0.056909140199422836\n",
      "[step: 384] loss: 0.06956321001052856\n",
      "[step: 385] loss: 0.05508992075920105\n",
      "[step: 386] loss: 0.06636875122785568\n",
      "[step: 387] loss: 0.07139710336923599\n",
      "[step: 388] loss: 0.06614593416452408\n",
      "[step: 389] loss: 0.05413707718253136\n",
      "[step: 390] loss: 0.07247669994831085\n",
      "[step: 391] loss: 0.05914561077952385\n",
      "[step: 392] loss: 0.04753538593649864\n",
      "[step: 393] loss: 0.05818605050444603\n",
      "[step: 394] loss: 0.08906020224094391\n",
      "[step: 395] loss: 0.0781654566526413\n",
      "[step: 396] loss: 0.07308288663625717\n",
      "[step: 397] loss: 0.05973582714796066\n",
      "[step: 398] loss: 0.06191243603825569\n",
      "[step: 399] loss: 0.07747600972652435\n",
      "[step: 400] loss: 0.07615320384502411\n",
      "[step: 401] loss: 0.0722237378358841\n",
      "[step: 402] loss: 0.08021442592144012\n",
      "[step: 403] loss: 0.06268039345741272\n",
      "[step: 404] loss: 0.05969785153865814\n",
      "[step: 405] loss: 0.08097235858440399\n",
      "[step: 406] loss: 0.06798483431339264\n",
      "[step: 407] loss: 0.060823891311883926\n",
      "[step: 408] loss: 0.06662078201770782\n",
      "[step: 409] loss: 0.07827483862638474\n",
      "[step: 410] loss: 0.0799686461687088\n",
      "[step: 411] loss: 0.06287027895450592\n",
      "[step: 412] loss: 0.0691327303647995\n",
      "[step: 413] loss: 0.052658915519714355\n",
      "[step: 414] loss: 0.04927252233028412\n",
      "[step: 415] loss: 0.05494631081819534\n",
      "[step: 416] loss: 0.0614682212471962\n",
      "[step: 417] loss: 0.07971486449241638\n",
      "[step: 418] loss: 0.0651247650384903\n",
      "[step: 419] loss: 0.04968276247382164\n",
      "[step: 420] loss: 0.04684600606560707\n",
      "[step: 421] loss: 0.08151081204414368\n",
      "[step: 422] loss: 0.0486217737197876\n",
      "[step: 423] loss: 0.05309557169675827\n",
      "[step: 424] loss: 0.06312620639801025\n",
      "[step: 425] loss: 0.072945736348629\n",
      "[step: 426] loss: 0.05121302604675293\n",
      "[step: 427] loss: 0.05128895491361618\n",
      "[step: 428] loss: 0.04814158380031586\n",
      "[step: 429] loss: 0.05548105388879776\n",
      "[step: 430] loss: 0.06680992245674133\n",
      "[step: 431] loss: 0.07169150561094284\n",
      "[step: 432] loss: 0.06135079637169838\n",
      "[step: 433] loss: 0.06946556270122528\n",
      "[step: 434] loss: 0.07057718932628632\n",
      "[step: 435] loss: 0.08544586598873138\n",
      "[step: 436] loss: 0.06717891246080399\n",
      "[step: 437] loss: 0.07583142817020416\n",
      "[step: 438] loss: 0.04924720525741577\n",
      "[step: 439] loss: 0.0637241005897522\n",
      "[step: 440] loss: 0.06280747056007385\n",
      "[step: 441] loss: 0.0619988888502121\n",
      "[step: 442] loss: 0.0598941445350647\n",
      "[step: 443] loss: 0.055056847631931305\n",
      "[step: 444] loss: 0.09577998518943787\n",
      "[step: 445] loss: 0.06341689079999924\n",
      "[step: 446] loss: 0.07434101402759552\n",
      "[step: 447] loss: 0.08336612582206726\n",
      "[step: 448] loss: 0.055862151086330414\n",
      "[step: 449] loss: 0.06446367502212524\n",
      "[step: 450] loss: 0.06686575710773468\n",
      "[step: 451] loss: 0.04868500679731369\n",
      "[step: 452] loss: 0.05548381805419922\n",
      "[step: 453] loss: 0.05204613506793976\n",
      "[step: 454] loss: 0.08130878210067749\n",
      "[step: 455] loss: 0.07354921847581863\n",
      "[step: 456] loss: 0.06838372349739075\n",
      "[step: 457] loss: 0.10871478170156479\n",
      "[step: 458] loss: 0.08579254150390625\n",
      "[step: 459] loss: 0.06891106069087982\n",
      "[step: 460] loss: 0.0645904541015625\n",
      "[step: 461] loss: 0.07108777016401291\n",
      "[step: 462] loss: 0.052234046161174774\n",
      "[step: 463] loss: 0.07198698818683624\n",
      "[step: 464] loss: 0.05736088380217552\n",
      "[step: 465] loss: 0.06882258504629135\n",
      "[step: 466] loss: 0.06217952072620392\n",
      "[step: 467] loss: 0.06824806332588196\n",
      "[step: 468] loss: 0.058373138308525085\n",
      "[step: 469] loss: 0.06849133223295212\n",
      "[step: 470] loss: 0.06006157025694847\n",
      "[step: 471] loss: 0.04804769158363342\n",
      "[step: 472] loss: 0.054818689823150635\n",
      "[step: 473] loss: 0.08374904096126556\n",
      "[step: 474] loss: 0.07104663550853729\n",
      "[step: 475] loss: 0.07600449025630951\n",
      "[step: 476] loss: 0.055720653384923935\n",
      "[step: 477] loss: 0.0572170689702034\n",
      "[step: 478] loss: 0.07098113745450974\n",
      "[step: 479] loss: 0.06335943192243576\n",
      "[step: 480] loss: 0.07395364344120026\n",
      "[step: 481] loss: 0.0632670447230339\n",
      "[step: 482] loss: 0.07122856378555298\n",
      "[step: 483] loss: 0.057550281286239624\n",
      "[step: 484] loss: 0.06276240944862366\n",
      "[step: 485] loss: 0.07242795825004578\n",
      "[step: 486] loss: 0.05121343955397606\n",
      "[step: 487] loss: 0.06698822975158691\n",
      "[step: 488] loss: 0.06395577639341354\n",
      "[step: 489] loss: 0.08301329612731934\n",
      "[step: 490] loss: 0.06622324138879776\n",
      "[step: 491] loss: 0.058332763612270355\n",
      "[step: 492] loss: 0.04421631991863251\n",
      "[step: 493] loss: 0.04688612371683121\n",
      "[step: 494] loss: 0.04531082510948181\n",
      "[step: 495] loss: 0.06277967989444733\n",
      "[step: 496] loss: 0.07577436417341232\n",
      "[step: 497] loss: 0.05542229861021042\n",
      "[step: 498] loss: 0.0500815249979496\n",
      "[step: 499] loss: 0.04890718311071396\n",
      "Test accuracy: 0.829\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "step_loss = 999999.0\n",
    "model_path = '../../models/RNN/my_RNN_model_S9_10'\n",
    "saver = tf.train.Saver()\n",
    "training_epochs = 500\n",
    "# Training step\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "for learning_rate in [0.02,0.01]:\n",
    "    test_acc = []\n",
    "    feed = {lr:learning_rate}\n",
    "    for i in range(training_epochs):\n",
    "        x,y = traindata.getBatchData()\n",
    "        feed[X], feed[Y] = x, y\n",
    "        step_loss_prev = step_loss\n",
    "        _, step_loss = sess.run([train, cost], feed_dict=feed)\n",
    "        cost_history = np.append(cost_history,step_loss)\n",
    "        \n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "        #batch_acc, test_state = sess.run([loss, _states], feed_dict=feed)\n",
    "    print(\"Test accuracy: {:.3f}\".format(1.0-np.mean(cost_history)))\n",
    "\n",
    "saver.save(sess, model_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../models/RNN/my_RNN_model_S9_10\n",
      "(1024,) (1024,)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver.restore(sess, model_path)\n",
    "x , y = testdata.getBatchData()\n",
    "y_pred = sess.run(tf.argmax(Y_pred,1),feed_dict={X: x})\n",
    "y_true = sess.run(tf.argmax(y,1))\n",
    "print(y_pred.shape, y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Score: 0.976\n",
      "Accuracy:  0.9755859375\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98       225\n",
      "          1       0.97      0.99      0.98       515\n",
      "          2       0.97      0.94      0.96       284\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1024\n",
      "\n",
      "[[222   0   3]\n",
      " [  1 510   4]\n",
      " [  3  14 267]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(cost_history)\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Iterations\") \n",
    "plt.axis([0,len(cost_history),0,np.max(cost_history)])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "p,r,f,s = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "print(\"F-Score:\", round(f,3))\n",
    "print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_path_f = \\'../models/RNN/\\'\\nfilename = \\'my_RNN_model_S9_40.meta\\'\\n\\n\\nsess = tf.InteractiveSession()\\nsess.run(tf.global_variables_initializer())\\nloader = tf.train.import_meta_graph(model_path_f+filename)\\nloader.restore(sess, tf.train.latest_checkpoint(model_path_f))\\n\\nSR = 22050\\n####\\njustone = True\\n\\nwhile(justone):\\n    justone = False\\n    #print(\"start to record the audio.\")\\n    \\n    frames = []\\n    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\\n        data = stream.read(CHUNK)\\n        frames.append(data)\\n    #print(\"Recording finished.\")\\n    stream.stop_stream()\\n    stream.close()\\n\\n    p.terminate()\\n\\n    wf = wave.open(WAVE_OUTPUT_FILENAME, \\'wb\\')\\n    wf.setnchannels(CHANNELS)\\n    wf.setsampwidth(p.get_sample_size(FORMAT))\\n    wf.setframerate(RATE)\\n    wf.writeframes(b\\'\\'.join(frames))\\n    wf.close()\\n    \\n    ####\\n    filename1 = \\'../data/phantom/JUNE_01_PHANTOMS/wavs/22050/WSU_P2_LOADED_BACK_AND_FORTH.wav\\'\\n    filename2 = \\'../data/phantom/JUNE_02_BACKGROUND/wavs/background/canopy_heavy_wind.wav\\'\\n    \\n    sample, sample_rate = librosa.load(filename1,SR)\\n    print(sample.shape)\\n    \\n    \\n    freqs, times, spectrogram = log_specgram(sample, sample_rate)    \\n    #showFreqTime([[sample, filename1, SR]])  \\n\\n    spectrogram = (spectrogram - mean) / std\\n    \\n    dataX = spectrogram\\n    #print(dataX.shape)\\n    #print(\\'delta shape:\\',dataX.shape)\\n\\n    X_hot_list= []\\n    #print(dataX.shape[0] - seq_length+1)\\n    for i in range(0, dataX.shape[0] - seq_length+1):\\n        _x = dataX[i:i + seq_length]\\n        X_hot_list.append(_x)\\n    X_hot = np.array(X_hot_list[:])\\n    #print(X_hot[0])\\n    #print(\\'\\n\\n\\n\\')\\n    y_pred = sess.run(Y_pred,feed_dict={X: X_hot})\\n    #y_pred[y_pred<0.5] = 0\\n    #y_pred[y_pred>=0.5] = 1\\n    print(y_pred[20:30] )\\n    y_true = np.ones(shape=[y_pred.shape[0]])\\n    y_pred[y_pred<0.5] = 0\\n    y_pred[y_pred>=0.5] = 1\\n    \\n    p,r,f,s = precision_recall_fscore_support(y_true, y_pred, average=\\'micro\\')\\n    print(\"F-Score:\", round(f,3))\\n    print(\"Accuracy: \", accuracy_score(y_true, y_pred))\\n\\n    print(classification_report(y_true, y_pred))\\n    print(confusion_matrix(y_true, y_pred))\\n\\n    \\n    if y_pred[0] == 1:\\n        print(\\'The sound is Drone\\')\\n    else :\\n        print(\\'THe sound isn\\'t Drone\\')\\n    \\n\\nsess.close()\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model_path_f = '../models/RNN/'\n",
    "filename = 'my_RNN_model_S9_40.meta'\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "loader = tf.train.import_meta_graph(model_path_f+filename)\n",
    "loader.restore(sess, tf.train.latest_checkpoint(model_path_f))\n",
    "\n",
    "SR = 22050\n",
    "####\n",
    "justone = True\n",
    "\n",
    "while(justone):\n",
    "    justone = False\n",
    "    #print(\"start to record the audio.\")\n",
    "    \n",
    "    frames = []\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    #print(\"Recording finished.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "\n",
    "    p.terminate()\n",
    "\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    \n",
    "    ####\n",
    "    filename1 = '../data/phantom/JUNE_01_PHANTOMS/wavs/22050/WSU_P2_LOADED_BACK_AND_FORTH.wav'\n",
    "    filename2 = '../data/phantom/JUNE_02_BACKGROUND/wavs/background/canopy_heavy_wind.wav'\n",
    "    \n",
    "    sample, sample_rate = librosa.load(filename1,SR)\n",
    "    print(sample.shape)\n",
    "    \n",
    "    \n",
    "    freqs, times, spectrogram = log_specgram(sample, sample_rate)    \n",
    "    #showFreqTime([[sample, filename1, SR]])  \n",
    "\n",
    "    spectrogram = (spectrogram - mean) / std\n",
    "    \n",
    "    dataX = spectrogram\n",
    "    #print(dataX.shape)\n",
    "    #print('delta shape:',dataX.shape)\n",
    "\n",
    "    X_hot_list= []\n",
    "    #print(dataX.shape[0] - seq_length+1)\n",
    "    for i in range(0, dataX.shape[0] - seq_length+1):\n",
    "        _x = dataX[i:i + seq_length]\n",
    "        X_hot_list.append(_x)\n",
    "    X_hot = np.array(X_hot_list[:])\n",
    "    #print(X_hot[0])\n",
    "    #print('\\n\\n\\n')\n",
    "    y_pred = sess.run(Y_pred,feed_dict={X: X_hot})\n",
    "    #y_pred[y_pred<0.5] = 0\n",
    "    #y_pred[y_pred>=0.5] = 1\n",
    "    print(y_pred[20:30] )\n",
    "    y_true = np.ones(shape=[y_pred.shape[0]])\n",
    "    y_pred[y_pred<0.5] = 0\n",
    "    y_pred[y_pred>=0.5] = 1\n",
    "    \n",
    "    p,r,f,s = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    print(\"F-Score:\", round(f,3))\n",
    "    print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "    \n",
    "    if y_pred[0] == 1:\n",
    "        print('The sound is Drone')\n",
    "    else :\n",
    "        print('THe sound isn\\'t Drone')\n",
    "    \n",
    "\n",
    "sess.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
